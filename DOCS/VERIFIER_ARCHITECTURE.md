# Verifier Block: Detailed Architecture Specification

## Overview

The Verifier Block represents the "immune system" of DELLM - the trust and quality assurance layer that validates answers, builds consensus, calculates fitness, and ensures the integrity of the entire network.

**CRITICAL: The Verifier Block CONTAINS a lightweight LLM (3B-7B parameters).** This makes it the third block type in DELLM that performs LLM inference, alongside Super LLM (70B+) and Node (1B-13B).

**Why Verifiers Need an LLM:**
- **Semantic answer validation** - Understanding meaning, not just syntax
- **Context-aware correctness** - Evaluating answers in context
- **Nuanced quality assessment** - Detecting subtleties in answer quality
- **Creative/open-ended answers** - No programmatic validation possible
- **Hallucination detection** - Identifying plausible but incorrect answers
- **Intelligent aggregation** - Combining diverse answers meaningfully

**Biological Metaphor:**
- **Verifier** = Immune system (identifies and eliminates problems)
- **Node** = Cell (basic unit)
- **Cluster** = Organ (coordinated group)
- **Super Cluster** = Nervous system (coordination)
- **Super LLM** = Brain (intelligence)

---

## Updated DELLM LLM Distribution

With Verifiers containing LLMs, DELLM now has **3 block types with LLMs**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COORDINATION LAYER                             â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  ðŸ§  Super LLM Block               â”‚         â”‚
â”‚  â”‚  âœ… CONTAINS LLM (70B+)           â”‚         â”‚
â”‚  â”‚  - Query decomposition            â”‚         â”‚
â”‚  â”‚  - Final synthesis                â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  NETWORK LAYER                                  â”‚
â”‚                                                 â”‚
â”‚  Super Cluster Block                            â”‚
â”‚  âŒ NO LLM (routing logic)                      â”‚
â”‚            â†“                                    â”‚
â”‚  Cluster Block                                  â”‚
â”‚  âŒ NO LLM (coordination logic)                 â”‚
â”‚            â†“                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  ðŸ’» Node Block                    â”‚         â”‚
â”‚  â”‚  âœ… CONTAINS LLM (1B-13B)         â”‚         â”‚
â”‚  â”‚  - Task execution                 â”‚         â”‚
â”‚  â”‚  - Answer generation              â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VERIFICATION LAYER                             â”‚
â”‚                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚  ðŸ” Verifier Block                â”‚         â”‚
â”‚  â”‚  âœ… CONTAINS LLM (3B-7B)          â”‚  â† NEW! â”‚
â”‚  â”‚  - Semantic validation            â”‚         â”‚
â”‚  â”‚  - Consensus building             â”‚         â”‚
â”‚  â”‚  - Quality assessment             â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**LLM Size Rationale:**
- **Super LLM (70B+):** Complex reasoning, decomposition, synthesis
- **Verifier (3B-7B):** Semantic understanding, quality assessment
- **Node (1B-13B):** Task execution on consumer devices

---

## Verifier Identity & State

### Unique Identifier & Configuration

```python
VerifierIdentity = {
    # Unique Verifier ID
    verifier_id: "verifier-tier2-cluster-math-001",
    
    # Verification Tier
    tier: "tier1" | "tier2" | "tier3" | "tier4",
    # - tier1: Node-level (self-validation, no dedicated verifier)
    # - tier2: Cluster-level (dedicated verifier with 3.8B LLM)
    # - tier3: Super Cluster-level (dedicated verifier with 7B LLM)
    # - tier4: Super LLM-level (integrated in synthesis)
    
    # Scope
    scope: "cluster" | "super_cluster",
    assigned_to: "cluster-math-specialists-001" | "super-cluster-main",
    
    # Model Configuration (for tier2 and tier3)
    model_config: {
        # Lightweight LLM for semantic validation
        model_name: "phi-3-mini-3.8b" | "mistral-7b-instruct",
        model_size_params: 3_800_000_000,  # 3.8B for tier2, 7B for tier3
        quantization: "Q4_K_M",  # 4-bit quantization
        
        # Hardware requirements
        hardware: {
            gpu: "RTX 3060 12GB" | "RTX 4070 16GB",  # Tier2 | Tier3
            vram_gb: 8 | 16,
            cpu_cores: 8 | 16,
            ram_gb: 16 | 32
        },
        
        # Inference configuration
        inference_engine: "llama.cpp" | "vllm",
        max_context_length: 4096,
        
        # Sampling parameters (for validation tasks)
        validation_sampling: {
            temperature: 0.1,  # Very low for deterministic validation
            top_p: 0.9,
            top_k: 40,
            max_tokens: 512  # Short validation responses
        }
    },
    
    # Specialization
    specialization: {
        primary_domains: ["mathematical_reasoning", "logic"],
        task_types: ["calculation", "proof", "equation_solving"],
        
        # Verification strategies per task type
        strategies: {
            "calculation": ["programmatic_execution", "ground_truth", "llm_validation"],
            "creative_writing": ["llm_validation", "quality_assessment"],
            "factual_qa": ["ground_truth", "cross_reference", "llm_validation"],
            "code_generation": ["programmatic_execution", "syntax_check", "llm_validation"]
        }
    },
    
    # Creation Metadata
    created_at: timestamp,
    version: "1.0.0"
}
```

### Runtime State

```python
VerifierRuntimeState = {
    # Execution Status
    status: "active" | "validating" | "maintenance" | "offline",
    current_validation: "validation-abc123" | null,
    
    # Validation Queue
    validation_queue: {
        pending_validations: 23,
        executing_validations: 5,
        completed_validations: 15847,
        failed_validations: 234
    },
    
    # Performance Metrics
    performance: {
        # Validation accuracy
        validation_accuracy: 0.96,  # 96% accuracy in correctness assessment
        false_positive_rate: 0.02,  # 2% incorrect approvals
        false_negative_rate: 0.02,  # 2% incorrect rejections
        
        # Throughput
        average_validation_latency: 1.8,  # Seconds per validation
        validations_per_hour: 2000,
        
        # Consensus building
        consensus_build_rate: 0.94,  # 94% successful consensus
        consensus_methods: {
            "exact_match": 0.67,      # 67% unanimous agreement
            "semantic_similarity": 0.21,  # 21% similar answers aggregated
            "weighted_majority": 0.09,    # 9% majority vote
            "llm_arbitration": 0.03       # 3% LLM resolved conflicts
        }
    },
    
    # Validation Statistics
    validation_stats: {
        total_validations: 15847,
        
        # By outcome
        approved: 14523,      # 91.6% pass
        rejected: 892,        # 5.6% fail
        uncertain: 432,       # 2.7% escalated
        
        # By method
        by_method: {
            "programmatic": 5234,      # 33%
            "ground_truth": 3892,      # 24.6%
            "llm_validation": 4523,    # 28.5%
            "cross_reference": 2198    # 13.9%
        },
        
        # Quality
        high_confidence_validations: 14234,  # >0.90 confidence
        low_confidence_validations: 1613     # <0.70 confidence
    },
    
    # Consensus Building Stats
    consensus_stats: {
        unanimous_consensus: 10617,    # 67%
        majority_consensus: 3562,      # 22.5%
        split_decisions: 1668          # 10.5%
    },
    
    # LLM State (for tier2/tier3)
    llm_state: {
        model_loaded: true,
        inference_latency_avg: 1.2,  # Seconds
        tokens_processed: 2847234,
        validation_calls: 8945
    },
    
    # Ground Truth Database
    ground_truth_db: {
        total_entries: 125000,
        categories: ["math_facts", "historical_dates", "scientific_constants", "code_outputs"],
        last_updated: timestamp,
        hit_rate: 0.38  # 38% of queries have ground truth
    },
    
    # Resource Utilization
    resources: {
        gpu_usage: 0.62,      # 62% GPU utilization
        vram_usage: 6.8,      # GB
        cpu_usage: 0.45,
        ram_usage: 12.3       # GB
    }
}
```

---

## Input Interfaces

### 1. Validation Request (from Cluster or Super Cluster)

**Primary Input:** Request to validate node answers and build consensus

```python
ValidationRequest = {
    # Identification
    request_id: "validation-req-abc123",
    task_id: "task-001",
    batch_id: "batch-xyz789",
    
    # Requester
    requested_by: "cluster-math-specialists-001" | "super-cluster-main",
    requester_type: "cluster" | "super_cluster",
    
    # Original Task Context
    original_task: {
        query: "Calculate the compound interest on $10,000 at 5% annually for 10 years",
        classification: {
            task_type: "calculation",
            domain: "mathematics",
            complexity: "simple"
        },
        expected_format: "numerical_answer" | "explanation" | "code" | "creative",
        ground_truth: "$6,288.95" | null  # If known
    },
    
    # Node Answers to Validate
    node_answers: [
        {
            node_id: "node-550e8400",
            answer: "The compound interest is $6,288.95",
            confidence: 0.92,
            reasoning: "Used formula A = P(1+r)^t = 10000(1.05)^10 = 16288.95, interest = 16288.95 - 10000 = 6288.95",
            latency: 1.8,
            ensemble_weight: 1.2,  # Node's voting weight
            node_type: "math_specialist",
            node_fitness: 0.87
        },
        {
            node_id: "node-661f9511",
            answer: "The compound interest is $6,288.95",
            confidence: 0.88,
            reasoning: "Calculated using compound interest formula",
            latency: 2.1,
            ensemble_weight: 1.1,
            node_type: "math_specialist",
            node_fitness: 0.82
        },
        {
            node_id: "node-772g0622",
            answer: "The compound interest is approximately $6,289",
            confidence: 0.85,
            reasoning: "Rounded calculation",
            latency: 2.3,
            ensemble_weight: 1.0,
            node_type: "generalist",
            node_fitness: 0.80
        }
    ],
    
    # Verification Requirements
    verification_requirements: {
        verification_level: "standard" | "high" | "critical",
        consensus_threshold: 0.60,  # 60% agreement required
        use_ground_truth: true,
        use_llm_validation: true,
        timeout_seconds: 5.0
    },
    
    # Metadata
    timestamp: 1640995202.5,
    priority: "normal" | "high"
}
```

**Input Channel:** gRPC RPC from Cluster or Super Cluster

---

## Output Interfaces

### 1. Verification Result (to Cluster or Super Cluster)

**Primary Output:** Consensus answer with fitness updates

```python
VerificationResult = {
    # Identification
    verification_id: "verify-abc123",
    request_id: "validation-req-abc123",
    task_id: "task-001",
    verifier_id: "verifier-tier2-cluster-math-001",
    
    # Consensus Answer (PRIMARY OUTPUT)
    consensus_answer: {
        # The aggregated answer
        answer: "The compound interest is $6,288.95",
        confidence: 0.88,  # Average confidence
        
        # How consensus was reached
        consensus_method: "exact_match_unanimous" | "weighted_majority" | 
                         "semantic_aggregation" | "llm_arbitration",
        
        consensus_strength: {
            agreement_percentage: 67,  # 2 of 3 exact match
            weighted_agreement: 76,    # Weighted by ensemble_weight
            dissenting_nodes: 1
        },
        
        # Aggregated reasoning (combined from nodes)
        aggregated_reasoning: "All nodes used compound interest formula A = P(1+r)^t. Two nodes calculated exact value $6,288.95, one rounded to $6,289.",
        
        # Quality assessment
        quality_assessment: {
            correctness: 1.0,  # Verified correct
            completeness: 0.95,
            clarity: 0.90,
            overall_quality: 0.95
        }
    },
    
    # Verification Details
    verification_details: {
        # Primary verification method used
        primary_method: "programmatic_execution" | "ground_truth" | 
                       "llm_validation" | "cross_reference",
        
        # Programmatic verification (if applicable)
        programmatic_verification: {
            executed: true,
            code: "P=10000; r=0.05; t=10; A=P*(1+r)**t; interest=A-P",
            result: 6288.95,
            matches_consensus: true,
            execution_time: 0.03
        } | null,
        
        # LLM validation (if used)
        llm_validation: {
            performed: true,
            llm_assessment: {
                correctness: 0.98,
                reasoning: "The calculation is mathematically correct. Formula properly applied.",
                confidence: 0.96
            },
            llm_inference_time: 1.2
        } | null,
        
        # Ground truth check (if available)
        ground_truth_check: {
            available: true,
            ground_truth_value: "$6,288.95",
            matches_consensus: true
        } | null,
        
        # Cross-reference check (if performed)
        cross_reference_check: {
            performed: false,
            sources_checked: 0
        } | null
    },
    
    # Fitness Updates (for each node)
    fitness_updates: [
        {
            node_id: "node-550e8400",
            
            verification_outcome: {
                answer_correct: true,
                reasoning_quality: "excellent",
                latency_rating: "fast",
                confidence_calibration: "well_calibrated"  # Confidence matched correctness
            },
            
            fitness_calculation: {
                old_fitness: 0.87,
                
                # Fitness components
                correctness_reward: +0.05,  # Correct answer
                speed_bonus: +0.01,         # Fast response (1.8s)
                reasoning_bonus: +0.01,     # High-quality reasoning
                confidence_penalty: 0.0,    # Well-calibrated
                
                # New fitness
                total_delta: +0.07,
                new_fitness: 0.94,
                clamped_fitness: 0.94  # Clamped to [0.0, 1.0]
            },
            
            credits_earned: {
                amount: 48,
                calculation: {
                    base_rate: 30,
                    quality_multiplier: 1.3,  # Excellent quality
                    speed_multiplier: 1.1,    # Fast
                    fitness_multiplier: 1.07  # High fitness
                }
            },
            
            feedback: "Excellent work! Correct answer with clear reasoning and fast execution."
        },
        
        {
            node_id: "node-661f9511",
            verification_outcome: {
                answer_correct: true,
                reasoning_quality: "good",
                latency_rating: "average",
                confidence_calibration: "well_calibrated"
            },
            fitness_calculation: {
                old_fitness: 0.82,
                correctness_reward: +0.05,
                speed_bonus: 0.0,
                reasoning_bonus: +0.005,
                confidence_penalty: 0.0,
                total_delta: +0.055,
                new_fitness: 0.875,
                clamped_fitness: 0.875
            },
            credits_earned: {
                amount: 42,
                calculation: {
                    base_rate: 30,
                    quality_multiplier: 1.2,
                    speed_multiplier: 1.0,
                    fitness_multiplier: 1.055
                }
            },
            feedback: "Correct answer! Consider providing more detailed reasoning."
        },
        
        {
            node_id: "node-772g0622",
            verification_outcome: {
                answer_correct: true,  # Rounded but close enough
                reasoning_quality: "minimal",
                latency_rating: "slow",
                confidence_calibration: "well_calibrated"
            },
            fitness_calculation: {
                old_fitness: 0.80,
                correctness_reward: +0.03,  # Rounded answer (partial credit)
                speed_bonus: -0.005,        # Slow response
                reasoning_bonus: 0.0,       # Minimal reasoning
                confidence_penalty: 0.0,
                total_delta: +0.025,
                new_fitness: 0.825,
                clamped_fitness: 0.825
            },
            credits_earned: {
                amount: 35,
                calculation: {
                    base_rate: 30,
                    quality_multiplier: 1.1,
                    speed_multiplier: 0.95,
                    fitness_multiplier: 1.025
                }
            },
            feedback: "Answer rounded but acceptable. Try to provide exact values and improve response time."
        }
    ],
    
    # Cluster-level Feedback (if requested by cluster)
    cluster_feedback: {
        cluster_id: "cluster-math-specialists-001",
        overall_performance: "excellent",
        
        cluster_quality: {
            consensus_quality: 0.95,
            average_node_quality: 0.91,
            consistency: 0.88  # How consistent nodes were
        },
        
        recommendations: [
            "Maintain current specialization focus",
            "Consider adding more detailed reasoning to all answers"
        ]
    } | null,
    
    # Execution Metadata
    execution: {
        total_verification_time: 2.1,  # Seconds
        
        time_breakdown: {
            consensus_building: 0.5,
            programmatic_verification: 0.03,
            llm_validation: 1.2,
            fitness_calculation: 0.3,
            result_packaging: 0.07
        },
        
        methods_used: ["programmatic_execution", "llm_validation"],
        llm_tokens_processed: 567
    },
    
    # Timestamp
    timestamp: 1640995204.6,
    verified_at: 1640995204.6
}
```

**Output Channel:** gRPC response to requester (Cluster or Super Cluster)

---

## Internal Mechanisms

### 1. Consensus Builder (PRIMARY TIER 2 FUNCTION)

**The most critical function: Aggregate multiple node answers into single consensus**

```python
class ConsensusBuilder:
    def build_consensus(self, node_answers: List[NodeAnswer]) -> ConsensusAnswer:
        """
        PRIMARY FUNCTION: Build consensus from node answers
        
        This is THE CORE of Tier 2 verification
        """
        # Step 1: Check for exact matches
        exact_matches = self.find_exact_matches(node_answers)
        
        if len(exact_matches) > 0 and len(exact_matches[0]) >= len(node_answers) * 0.6:
            # 60%+ exact agreement - unanimous or strong majority
            return self.aggregate_exact_matches(exact_matches[0])
        
        # Step 2: Check for semantic similarity (using verifier LLM)
        semantic_groups = self.group_by_semantic_similarity(node_answers)
        
        if len(semantic_groups) > 0:
            largest_group = max(semantic_groups, key=lambda g: sum(a.ensemble_weight for a in g))
            
            # Check if largest group has sufficient weighted agreement
            total_weight = sum(a.ensemble_weight for a in node_answers)
            group_weight = sum(a.ensemble_weight for a in largest_group)
            
            if group_weight / total_weight >= 0.6:
                # Semantic consensus reached
                return self.aggregate_similar_answers(largest_group)
        
        # Step 3: Weighted majority (ensemble weights matter)
        weighted_consensus = self.weighted_majority_vote(node_answers)
        
        if weighted_consensus.agreement >= 0.5:
            return weighted_consensus
        
        # Step 4: LLM arbitration (no clear consensus)
        return self.llm_arbitration(node_answers)
    
    def find_exact_matches(self, node_answers: List[NodeAnswer]) -> List[List[NodeAnswer]]:
        """
        Group answers that are exactly the same (after normalization)
        """
        # Normalize answers (lowercase, strip whitespace, remove punctuation)
        normalized_groups = {}
        
        for answer in node_answers:
            normalized = self.normalize_answer(answer.answer)
            
            if normalized not in normalized_groups:
                normalized_groups[normalized] = []
            
            normalized_groups[normalized].append(answer)
        
        # Sort groups by size (largest first)
        groups = sorted(normalized_groups.values(), key=len, reverse=True)
        
        return groups
    
    def normalize_answer(self, answer: str) -> str:
        """
        Normalize answer for comparison
        """
        # Remove extra whitespace
        normalized = " ".join(answer.split())
        
        # Lowercase
        normalized = normalized.lower()
        
        # Remove common punctuation
        normalized = normalized.replace(",", "").replace(".", "").replace("$", "")
        
        # Handle numerical formats
        # "6288.95" and "6,288.95" and "$6288.95" should match
        normalized = re.sub(r'[,$]', '', normalized)
        
        return normalized.strip()
    
    def group_by_semantic_similarity(self, node_answers: List[NodeAnswer]) -> List[List[NodeAnswer]]:
        """
        Use verifier LLM to group semantically similar answers
        
        CRITICAL: This is where the verifier's LLM is essential
        """
        if len(node_answers) <= 1:
            return [node_answers]
        
        # Calculate pairwise similarity using LLM
        similarity_matrix = {}
        
        for i, answer_a in enumerate(node_answers):
            for j, answer_b in enumerate(node_answers):
                if i >= j:
                    continue
                
                # LLM-based semantic similarity
                similarity = self.calculate_semantic_similarity(
                    answer_a.answer, 
                    answer_b.answer
                )
                
                similarity_matrix[(i, j)] = similarity
        
        # Cluster answers by similarity (threshold: 0.90)
        groups = []
        assigned = set()
        
        for i, answer in enumerate(node_answers):
            if i in assigned:
                continue
            
            # Start new group
            group = [answer]
            assigned.add(i)
            
            # Find similar answers
            for j, other_answer in enumerate(node_answers):
                if j <= i or j in assigned:
                    continue
                
                if similarity_matrix.get((i, j), 0) >= 0.90:
                    group.append(other_answer)
                    assigned.add(j)
            
            groups.append(group)
        
        return groups
    
    def calculate_semantic_similarity(self, answer_a: str, answer_b: str) -> float:
        """
        Use verifier LLM to calculate semantic similarity (0.0-1.0)
        
        CRITICAL: This requires the verifier's lightweight LLM
        """
        prompt = f"""Compare these two answers for semantic similarity.

Answer A: "{answer_a}"
Answer B: "{answer_b}"

Are they saying the same thing in different words?

Respond with ONLY a number from 0.0 to 1.0:
- 1.0 = identical meaning
- 0.9 = very similar meaning
- 0.7 = somewhat similar
- 0.5 = related but different
- 0.3 = different
- 0.0 = completely different

Similarity score:"""
        
        response = self.llm_inference(
            prompt=prompt,
            temperature=0.1,  # Very low for consistency
            max_tokens=10
        )
        
        # Parse similarity score
        try:
            similarity = float(response.strip())
            return max(0.0, min(1.0, similarity))  # Clamp to [0.0, 1.0]
        except:
            return 0.0  # Default to no similarity on parse error
    
    def aggregate_similar_answers(self, similar_answers: List[NodeAnswer]) -> ConsensusAnswer:
        """
        Aggregate semantically similar answers using LLM
        
        CRITICAL: LLM selects the best phrasing
        """
        if len(similar_answers) == 1:
            return ConsensusAnswer(
                answer=similar_answers[0].answer,
                confidence=similar_answers[0].confidence,
                consensus_method="single_answer",
                consensus_strength={"agreement_percentage": 100}
            )
        
        # Use LLM to select best phrasing
        answers_text = "\n".join([
            f"{i+1}. {a.answer} (confidence: {a.confidence:.2f}, node: {a.node_type})"
            for i, a in enumerate(similar_answers)
        ])
        
        prompt = f"""Select the best phrasing from these semantically similar answers:

{answers_text}

Choose the answer that is:
1. Most clear and precise
2. Most complete
3. Best formatted

Respond with ONLY the number (1, 2, 3, etc.) of the best answer:"""
        
        response = self.llm_inference(prompt, temperature=0.1, max_tokens=5)
        
        try:
            selected_idx = int(response.strip()) - 1
            selected_answer = similar_answers[selected_idx]
        except:
            # Fallback: select highest confidence
            selected_answer = max(similar_answers, key=lambda a: a.confidence)
        
        # Calculate aggregated confidence (weighted average)
        total_weight = sum(a.ensemble_weight for a in similar_answers)
        weighted_confidence = sum(
            a.confidence * a.ensemble_weight 
            for a in similar_answers
        ) / total_weight
        
        # Calculate agreement
        agreement_percentage = (len(similar_answers) / len(similar_answers)) * 100
        
        return ConsensusAnswer(
            answer=selected_answer.answer,
            confidence=weighted_confidence,
            consensus_method="semantic_aggregation",
            consensus_strength={
                "agreement_percentage": agreement_percentage,
                "weighted_agreement": (total_weight / sum(a.ensemble_weight for a in similar_answers)) * 100
            },
            aggregated_reasoning=self.combine_reasoning(similar_answers)
        )
    
    def llm_arbitration(self, node_answers: List[NodeAnswer]) -> ConsensusAnswer:
        """
        LLM resolves conflicting answers when no clear consensus
        
        CRITICAL: This is the fallback when nodes disagree
        """
        answers_text = "\n\n".join([
            f"Node {i+1} ({a.node_type}, fitness: {a.node_fitness:.2f}):\n"
            f"Answer: {a.answer}\n"
            f"Confidence: {a.confidence:.2f}\n"
            f"Reasoning: {a.reasoning}"
            for i, a in enumerate(node_answers)
        ])
        
        prompt = f"""The nodes provided conflicting answers. You must arbitrate.

Task: {self.original_task.query}

Node Answers:
{answers_text}

Evaluate each answer and select the best one. Consider:
1. Correctness
2. Reasoning quality
3. Node fitness and expertise
4. Confidence calibration

Provide:
1. The best answer (copy it exactly)
2. Brief explanation of why it's best
3. Confidence score (0.0-1.0)

Format:
ANSWER: [answer]
REASONING: [why this is best]
CONFIDENCE: [0.0-1.0]"""
        
        response = self.llm_inference(prompt, temperature=0.3, max_tokens=512)
        
        # Parse LLM arbitration
        answer = self.extract_field(response, "ANSWER")
        reasoning = self.extract_field(response, "REASONING")
        confidence = float(self.extract_field(response, "CONFIDENCE"))
        
        return ConsensusAnswer(
            answer=answer,
            confidence=confidence,
            consensus_method="llm_arbitration",
            consensus_strength={
                "agreement_percentage": 0,  # No agreement, LLM decided
                "llm_confidence": confidence
            },
            aggregated_reasoning=reasoning
        )
```

---

### 2. Answer Validator

**Verify the consensus answer is actually correct**

```python
class AnswerValidator:
    def validate(
        self, 
        consensus_answer: ConsensusAnswer,
        original_task: Task,
        verification_requirements: Dict
    ) -> ValidationOutcome:
        """
        Validate the consensus answer using multiple methods
        """
        validation_results = {}
        
        # Method 1: Programmatic verification (if applicable)
        if original_task.classification.task_type in ["calculation", "code_execution"]:
            validation_results["programmatic"] = self.programmatic_verification(
                consensus_answer.answer,
                original_task
            )
        
        # Method 2: Ground truth check (if available)
        if verification_requirements.use_ground_truth:
            validation_results["ground_truth"] = self.ground_truth_verification(
                consensus_answer.answer,
                original_task
            )
        
        # Method 3: LLM validation (always available)
        if verification_requirements.use_llm_validation:
            validation_results["llm"] = self.llm_validation(
                consensus_answer.answer,
                original_task
            )
        
        # Aggregate validation results
        final_validation = self.aggregate_validation_results(validation_results)
        
        return final_validation
    
    def programmatic_verification(self, answer: str, task: Task) -> Dict:
        """
        Execute calculation or code to verify answer
        
        For math: Re-execute calculation
        For code: Run code and check output
        """
        if task.classification.task_type == "calculation":
            # Extract numerical answer
            numerical_answer = self.extract_number(answer)
            
            # Parse task and execute
            try:
                # Example: "Calculate 15% of 240"
                if "%" in task.query and "of" in task.query:
                    # Parse percentage calculation
                    parts = task.query.lower().split()
                    percentage = float(parts[parts.index("calculate") + 1].strip("%"))
                    of_idx = parts.index("of")
                    number = float(parts[of_idx + 1])
                    
                    correct_answer = (percentage / 100) * number
                    
                    # Check if answer matches (within tolerance)
                    matches = abs(numerical_answer - correct_answer) < 0.01
                    
                    return {
                        "method": "programmatic_execution",
                        "executed": True,
                        "calculated_value": correct_answer,
                        "provided_value": numerical_answer,
                        "matches": matches,
                        "correctness": 1.0 if matches else 0.0
                    }
            except Exception as e:
                return {
                    "method": "programmatic_execution",
                    "executed": False,
                    "error": str(e),
                    "correctness": None
                }
        
        elif task.classification.task_type == "code_execution":
            # Execute code and check output
            try:
                # Extract code from answer
                code = self.extract_code(answer)
                
                # Execute in sandbox
                result = self.execute_code_sandbox(code)
                expected = task.expected_output
                
                matches = result == expected
                
                return {
                    "method": "code_execution",
                    "executed": True,
                    "output": result,
                    "expected": expected,
                    "matches": matches,
                    "correctness": 1.0 if matches else 0.0
                }
            except Exception as e:
                return {
                    "method": "code_execution",
                    "executed": False,
                    "error": str(e),
                    "correctness": None
                }
        
        return {"method": "programmatic", "executed": False, "correctness": None}
    
    def ground_truth_verification(self, answer: str, task: Task) -> Dict:
        """
        Check against ground truth database
        """
        # Query ground truth database
        ground_truth = self.ground_truth_db.lookup(task.query)
        
        if ground_truth is None:
            return {
                "method": "ground_truth",
                "available": False,
                "correctness": None
            }
        
        # Normalize and compare
        normalized_answer = self.normalize_answer(answer)
        normalized_truth = self.normalize_answer(ground_truth)
        
        matches = normalized_answer == normalized_truth
        
        return {
            "method": "ground_truth",
            "available": True,
            "ground_truth_value": ground_truth,
            "matches": matches,
            "correctness": 1.0 if matches else 0.0
        }
    
    def llm_validation(self, answer: str, task: Task) -> Dict:
        """
        Use verifier LLM to assess answer quality and correctness
        
        CRITICAL: This is where the verifier's LLM is essential
        Particularly important for creative/open-ended answers
        """
        prompt = f"""Evaluate this answer for correctness and quality.

Task: {task.query}
Task Type: {task.classification.task_type}
Domain: {task.classification.domain}

Answer: {answer}

Evaluate:
1. Correctness (does it accurately answer the question?)
2. Completeness (are all aspects addressed?)
3. Clarity (is it well-explained?)
4. Format (proper structure and presentation?)

Rate each from 0.0 to 1.0, then provide overall assessment.

Format:
CORRECTNESS: [0.0-1.0]
COMPLETENESS: [0.0-1.0]
CLARITY: [0.0-1.0]
FORMAT: [0.0-1.0]
REASONING: [brief explanation]
OVERALL: [0.0-1.0]"""
        
        response = self.llm_inference(prompt, temperature=0.2, max_tokens=256)
        
        # Parse LLM evaluation
        correctness = float(self.extract_field(response, "CORRECTNESS"))
        completeness = float(self.extract_field(response, "COMPLETENESS"))
        clarity = float(self.extract_field(response, "CLARITY"))
        format_score = float(self.extract_field(response, "FORMAT"))
        reasoning = self.extract_field(response, "REASONING")
        overall = float(self.extract_field(response, "OVERALL"))
        
        return {
            "method": "llm_validation",
            "performed": True,
            "assessment": {
                "correctness": correctness,
                "completeness": completeness,
                "clarity": clarity,
                "format": format_score,
                "reasoning": reasoning,
                "overall": overall
            },
            "correctness": overall  # Use overall as correctness score
        }
    
    def aggregate_validation_results(self, validation_results: Dict) -> ValidationOutcome:
        """
        Combine multiple validation methods into final verdict
        
        Priority: Programmatic > Ground Truth > LLM
        """
        # Prioritize programmatic verification (most reliable)
        if "programmatic" in validation_results:
            prog = validation_results["programmatic"]
            if prog.get("executed") and prog.get("correctness") is not None:
                return ValidationOutcome(
                    verification_passed=prog["correctness"] >= 0.95,
                    correctness=prog["correctness"],
                    primary_method="programmatic_execution",
                    all_methods=validation_results
                )
        
        # Prioritize ground truth (very reliable)
        if "ground_truth" in validation_results:
            gt = validation_results["ground_truth"]
            if gt.get("available") and gt.get("correctness") is not None:
                return ValidationOutcome(
                    verification_passed=gt["correctness"] >= 0.95,
                    correctness=gt["correctness"],
                    primary_method="ground_truth",
                    all_methods=validation_results
                )
        
        # Use LLM validation (least reliable but always available)
        if "llm" in validation_results:
            llm = validation_results["llm"]
            if llm.get("performed"):
                return ValidationOutcome(
                    verification_passed=llm["correctness"] >= 0.75,
                    correctness=llm["correctness"],
                    primary_method="llm_validation",
                    all_methods=validation_results
                )
        
        # No validation possible - uncertain
        return ValidationOutcome(
            verification_passed=None,
            correctness=None,
            primary_method="none",
            all_methods=validation_results
        )
```

---

### 3. Fitness Calculator

**Calculate fitness updates and credits for each node**

```python
class FitnessCalculator:
    def calculate_fitness_updates(
        self,
        node_answers: List[NodeAnswer],
        consensus_answer: ConsensusAnswer,
        validation_outcome: ValidationOutcome
    ) -> List[FitnessUpdate]:
        """
        Calculate fitness updates for all nodes based on verification
        """
        fitness_updates = []
        
        for node_answer in node_answers:
            # Evaluate this node's answer
            evaluation = self.evaluate_node_answer(
                node_answer=node_answer,
                consensus_answer=consensus_answer,
                validation_outcome=validation_outcome
            )
            
            # Calculate fitness delta
            fitness_delta = self.calculate_fitness_delta(
                node_answer=node_answer,
                evaluation=evaluation
            )
            
            # Calculate credits earned
            credits = self.calculate_credits(
                node_answer=node_answer,
                evaluation=evaluation,
                fitness_delta=fitness_delta
            )
            
            # Generate feedback
            feedback = self.generate_feedback(
                node_answer=node_answer,
                evaluation=evaluation,
                fitness_delta=fitness_delta
            )
            
            fitness_updates.append(FitnessUpdate(
                node_id=node_answer.node_id,
                verification_outcome=evaluation,
                fitness_calculation={
                    "old_fitness": node_answer.node_fitness,
                    "correctness_reward": fitness_delta.correctness_reward,
                    "speed_bonus": fitness_delta.speed_bonus,
                    "reasoning_bonus": fitness_delta.reasoning_bonus,
                    "confidence_penalty": fitness_delta.confidence_penalty,
                    "total_delta": fitness_delta.total,
                    "new_fitness": min(1.0, max(0.0, node_answer.node_fitness + fitness_delta.total)),
                    "clamped_fitness": min(1.0, max(0.0, node_answer.node_fitness + fitness_delta.total))
                },
                credits_earned=credits,
                feedback=feedback
            ))
        
        return fitness_updates
    
    def evaluate_node_answer(
        self,
        node_answer: NodeAnswer,
        consensus_answer: ConsensusAnswer,
        validation_outcome: ValidationOutcome
    ) -> NodeEvaluation:
        """
        Evaluate how good this node's answer was
        """
        # Check if answer matches consensus
        answer_matches_consensus = self.normalize_answer(node_answer.answer) == \
                                   self.normalize_answer(consensus_answer.answer)
        
        # Determine correctness
        if validation_outcome.correctness is not None:
            # We have validated correctness
            answer_correct = answer_matches_consensus and validation_outcome.correctness >= 0.90
        else:
            # No validation - assume consensus is correct
            answer_correct = answer_matches_consensus
        
        # Evaluate reasoning quality
        reasoning_quality = self.evaluate_reasoning_quality(node_answer.reasoning)
        
        # Evaluate latency
        if node_answer.latency < 2.0:
            latency_rating = "fast"
        elif node_answer.latency < 3.5:
            latency_rating = "average"
        else:
            latency_rating = "slow"
        
        # Check confidence calibration
        # If confident and correct = good
        # If confident and wrong = bad (overconfident)
        # If uncertain and wrong = ok (appropriate uncertainty)
        if answer_correct and node_answer.confidence > 0.85:
            confidence_calibration = "well_calibrated"
        elif not answer_correct and node_answer.confidence > 0.85:
            confidence_calibration = "overconfident"
        elif not answer_correct and node_answer.confidence < 0.70:
            confidence_calibration = "appropriately_uncertain"
        else:
            confidence_calibration = "acceptable"
        
        return NodeEvaluation(
            answer_correct=answer_correct,
            reasoning_quality=reasoning_quality,
            latency_rating=latency_rating,
            confidence_calibration=confidence_calibration
        )
    
    def calculate_fitness_delta(
        self,
        node_answer: NodeAnswer,
        evaluation: NodeEvaluation
    ) -> FitnessDelta:
        """
        Calculate fitness delta based on evaluation
        
        Fitness Components:
        1. Correctness reward (Â±0.03 to Â±0.05) - PRIMARY FACTOR
        2. Speed bonus (0 to +0.01)
        3. Reasoning bonus (0 to +0.01)
        4. Confidence penalty (0 to -0.01) - for overconfidence
        """
        # Correctness reward (most important)
        if evaluation.answer_correct:
            if evaluation.reasoning_quality == "excellent":
                correctness_reward = +0.05
            elif evaluation.reasoning_quality == "good":
                correctness_reward = +0.045
            else:
                correctness_reward = +0.03
        else:
            # Wrong answer - penalty
            correctness_reward = -0.03
        
        # Speed bonus
        if evaluation.latency_rating == "fast":
            speed_bonus = +0.01
        elif evaluation.latency_rating == "average":
            speed_bonus = 0.0
        else:
            speed_bonus = -0.005
        
        # Reasoning bonus
        if evaluation.reasoning_quality == "excellent":
            reasoning_bonus = +0.01
        elif evaluation.reasoning_quality == "good":
            reasoning_bonus = +0.005
        else:
            reasoning_bonus = 0.0
        
        # Confidence penalty (for overconfidence)
        if evaluation.confidence_calibration == "overconfident":
            confidence_penalty = -0.01
        else:
            confidence_penalty = 0.0
        
        # Total delta
        total = correctness_reward + speed_bonus + reasoning_bonus + confidence_penalty
        
        return FitnessDelta(
            correctness_reward=correctness_reward,
            speed_bonus=speed_bonus,
            reasoning_bonus=reasoning_bonus,
            confidence_penalty=confidence_penalty,
            total=total
        )
    
    def calculate_credits(
        self,
        node_answer: NodeAnswer,
        evaluation: NodeEvaluation,
        fitness_delta: FitnessDelta
    ) -> Dict:
        """
        Calculate credits earned for this task
        
        Credits Formula:
        base_rate Ã— quality_multiplier Ã— speed_multiplier Ã— fitness_multiplier
        """
        base_rate = 30  # Base credits per task
        
        # Quality multiplier (0.5 to 1.5)
        if evaluation.answer_correct:
            if evaluation.reasoning_quality == "excellent":
                quality_multiplier = 1.5
            elif evaluation.reasoning_quality == "good":
                quality_multiplier = 1.3
            else:
                quality_multiplier = 1.1
        else:
            quality_multiplier = 0.5  # Wrong answer gets minimal credits
        
        # Speed multiplier (0.95 to 1.1)
        if evaluation.latency_rating == "fast":
            speed_multiplier = 1.1
        elif evaluation.latency_rating == "average":
            speed_multiplier = 1.0
        else:
            speed_multiplier = 0.95
        
        # Fitness multiplier (based on new fitness)
        new_fitness = min(1.0, max(0.0, node_answer.node_fitness + fitness_delta.total))
        fitness_multiplier = 0.8 + (new_fitness * 0.4)  # 0.8 to 1.2 range
        
        # Calculate total
        amount = int(base_rate * quality_multiplier * speed_multiplier * fitness_multiplier)
        
        return {
            "amount": amount,
            "calculation": {
                "base_rate": base_rate,
                "quality_multiplier": quality_multiplier,
                "speed_multiplier": speed_multiplier,
                "fitness_multiplier": fitness_multiplier
            }
        }
    
    def generate_feedback(
        self,
        node_answer: NodeAnswer,
        evaluation: NodeEvaluation,
        fitness_delta: FitnessDelta
    ) -> str:
        """
        Generate helpful feedback for the node
        """
        if evaluation.answer_correct:
            if evaluation.reasoning_quality == "excellent":
                return "Excellent work! Correct answer with clear reasoning and fast execution."
            elif evaluation.reasoning_quality == "good":
                return "Correct answer! Consider providing more detailed reasoning."
            else:
                return "Correct answer. Try to explain your reasoning more clearly."
        else:
            if evaluation.confidence_calibration == "overconfident":
                return "Incorrect answer. Be more careful and lower confidence when uncertain."
            else:
                return "Incorrect answer. Review your approach and verify calculations."
```

---

## Verification Workflow

**Complete verification process:**

```python
class VerifierBlock:
    def verify(self, request: ValidationRequest) -> VerificationResult:
        """
        Complete verification workflow
        """
        start_time = time.now()
        
        # Step 1: Build consensus from node answers
        print(f"[Verifier] Building consensus from {len(request.node_answers)} node answers...")
        consensus_answer = self.consensus_builder.build_consensus(request.node_answers)
        print(f"[Verifier] Consensus: '{consensus_answer.answer}' (method: {consensus_answer.consensus_method})")
        
        # Step 2: Validate the consensus answer
        print(f"[Verifier] Validating consensus answer...")
        validation_outcome = self.answer_validator.validate(
            consensus_answer=consensus_answer,
            original_task=request.original_task,
            verification_requirements=request.verification_requirements
        )
        print(f"[Verifier] Validation: {validation_outcome.verification_passed} (correctness: {validation_outcome.correctness:.2f})")
        
        # Step 3: Calculate fitness updates for each node
        print(f"[Verifier] Calculating fitness updates...")
        fitness_updates = self.fitness_calculator.calculate_fitness_updates(
            node_answers=request.node_answers,
            consensus_answer=consensus_answer,
            validation_outcome=validation_outcome
        )
        
        for update in fitness_updates:
            print(f"[Verifier] Node {update.node_id}: {update.fitness_calculation['old_fitness']:.3f} -> {update.fitness_calculation['new_fitness']:.3f} ({update.fitness_calculation['total_delta']:+.3f})")
        
        # Step 4: Assess overall quality
        quality_assessment = self.assess_quality(
            consensus_answer=consensus_answer,
            validation_outcome=validation_outcome
        )
        
        # Step 5: Package result
        result = VerificationResult(
            verification_id=generate_uuid(),
            request_id=request.request_id,
            task_id=request.task_id,
            verifier_id=self.verifier_id,
            consensus_answer={
                **consensus_answer,
                "quality_assessment": quality_assessment
            },
            verification_details={
                "primary_method": validation_outcome.primary_method,
                **validation_outcome.all_methods
            },
            fitness_updates=fitness_updates,
            execution={
                "total_verification_time": time.now() - start_time
            },
            timestamp=time.now()
        )
        
        print(f"[Verifier] Verification complete in {result.execution['total_verification_time']:.2f}s")
        
        return result
```

**Average latency breakdown:**
- Consensus building: 0.5s
- Programmatic verification: 0.03s
- LLM validation: 1.2s
- Fitness calculation: 0.3s
- Result packaging: 0.07s
- **Total: ~2.1s**

---

## Multi-Tier Verification Architecture

### Tier 1: Node-Level (Self-Validation)

**Location:** Within each Node Block
**Hardware:** Node's own resources (no additional hardware)
**LLM:** Node's own LLM (1B-13B params)
**Latency:** <0.1s
**Coverage:** 100% (all tasks)

**Functions:**
1. Basic format validation
2. Constraint checking (length, format, etc.)
3. Self-confidence assessment
4. Obvious error detection

**Implementation:**
```python
# Inside Node Block
def self_validate(self, answer: str) -> bool:
    """
    Quick self-validation before sending to cluster
    """
    # Check format
    if not self.check_format(answer):
        return False
    
    # Check constraints
    if len(answer.split()) > self.max_tokens:
        return False
    
    # Check for obvious errors
    if self.contains_obvious_errors(answer):
        return False
    
    return True
```

**Purpose:** Filter out obviously bad answers before they reach Tier 2

---

### Tier 2: Cluster-Level (PRIMARY VERIFICATION)

**Location:** Dedicated Verifier Block per Cluster
**Hardware:** 1x RTX 3060 (12GB VRAM), 8 CPU cores, 16GB RAM
**LLM:** phi-3-mini-3.8b (2.3GB model)
**Latency:** 1-3s per validation
**Coverage:** 100% (all cluster tasks)

**Functions (THE CORE):**
1. **Aggregate node answers** (MOST IMPORTANT)
   - Build consensus from multiple nodes
   - Semantic similarity checking
   - LLM arbitration for conflicts
   
2. **Verify correctness**
   - Programmatic execution (math, code)
   - Ground truth database lookup
   - LLM validation (semantic correctness)
   - Cross-reference checking
   
3. **Calculate fitness updates**
   - Reward correct answers
   - Penalize wrong answers
   - Consider speed, reasoning, confidence
   
4. **Provide feedback**
   - Constructive messages to help nodes improve

**Deployment:**
- 1 verifier per cluster
- Colocated with cluster coordinator
- Dedicated GPU (RTX 3060 12GB)
- Docker container

**Verification Methods:**

1. **Programmatic Execution:**
   - For: Calculations, code tasks
   - How: Execute calculation/code, compare result
   - Reliability: Very high (99%+)
   - Latency: 0.01-0.1s

2. **Ground Truth Database:**
   - For: Factual questions, known answers
   - How: Lookup in database (125K entries)
   - Reliability: Very high (99%+)
   - Latency: 0.01s
   - Coverage: ~38% of queries

3. **LLM Validation:**
   - For: Creative, open-ended, semantic tasks
   - How: Verifier LLM assesses quality/correctness
   - Reliability: High (96%)
   - Latency: 1-2s
   - Coverage: 100% (fallback method)

4. **Cross-Reference:**
   - For: Factual claims, citations
   - How: Check against external sources
   - Reliability: High (95%)
   - Latency: 0.5-2s

**Priority Order:**
Programmatic > Ground Truth > LLM Validation > Cross-Reference

---

### Tier 3: Super Cluster-Level (Spot Checks)

**Location:** Dedicated Verifier Blocks at Super Cluster
**Hardware:** 1x RTX 4070 (16GB VRAM), 16 CPU cores, 32GB RAM
**LLM:** mistral-7b-instruct (4GB model)
**Latency:** 2-5s per validation
**Coverage:** 10-20% (spot checks)

**Functions:**
1. **Spot check critical tasks**
   - High-stakes queries
   - Low-confidence cluster results
   - Random sampling
   
2. **Cross-cluster validation**
   - Compare results across clusters
   - Detect systematic biases
   
3. **High-stakes verification**
   - Medical, legal, financial queries
   - Increased redundancy and thoroughness
   
4. **Fraud detection**
   - Detect gaming/cheating by clusters
   - Verify verifier accuracy

**Deployment:**
- 3-5 verifiers per super cluster
- Dedicated high-end hardware
- Round-robin load balancing

**When to spot-check:**
- Random 10% of all tasks
- 100% of high-stakes tasks (medical, legal, financial)
- 50% of low-confidence results (<0.75)
- 20% of new cluster results (first 100 tasks)
- Tasks flagged by Tier 2 verifiers as uncertain

---

### Tier 4: Super LLM-Level (Final QA)

**Location:** Integrated in Super LLM Block
**Hardware:** Super LLM's hardware (4x A100)
**LLM:** Super LLM (70B+)
**Latency:** 1-2s (part of synthesis)
**Coverage:** 100% (all final answers)

**Functions:**
1. **Synthesis validation**
   - Verify synthesis is coherent
   - Check all subtask answers incorporated
   
2. **Logical consistency check**
   - Ensure no contradictions
   - Verify reasoning flow
   
3. **Completeness check**
   - All aspects of query addressed
   - No missing information
   
4. **Quality assurance before user delivery**
   - Final polish
   - Format check
   - User satisfaction optimization

**Implementation:**
```python
# Inside Super LLM synthesis
def synthesize_and_validate(self, subtask_results: List) -> FinalAnswer:
    # Synthesize
    answer = self.synthesize(subtask_results)
    
    # Validate
    validation = self.validate_synthesis(answer, subtask_results)
    
    if not validation.passed:
        # Re-synthesize with corrections
        answer = self.re_synthesize(answer, validation.issues)
    
    return answer
```

---

## Deployment & Scaling

### Tier 2 (Cluster-Level) Deployment

**Hardware per Verifier:**
```
GPU: 1x RTX 3060 (12GB VRAM) - $300-400
CPU: 8 cores
RAM: 16GB
Storage: 20GB SSD
Model: phi-3-mini-3.8b (2.3GB)
Power: ~150W

Total Cost per Verifier: ~$500-800 (one-time capex)
```

**Scaling Strategy:**
- 1 verifier per cluster
- Current: 55 clusters â†’ 55 verifiers
- Total capex: $27,500 - $44,000

**Deployment:**
- Colocated with cluster coordinator
- Docker container: `dellm/verifier:tier2-phi3`
- Config: cluster_id, model_path, gpu_id

**Resource Requirements:**
- VRAM: 8GB (model + activations)
- Latency: 1-3s per validation
- Throughput: 1200-2000 validations/hour

---

### Tier 3 (Super Cluster-Level) Deployment

**Hardware per Verifier:**
```
GPU: 1x RTX 4070 (16GB VRAM) - $600-800
CPU: 16 cores
RAM: 32GB
Storage: 50GB SSD
Model: mistral-7b-instruct (4GB)
Power: ~250W

Total Cost per Verifier: ~$1000-1500 (one-time capex)
```

**Scaling Strategy:**
- 3-5 verifiers per super cluster
- Current: 1 super cluster â†’ 5 verifiers
- Total capex: $5,000 - $7,500

**Deployment:**
- Dedicated verification servers
- Load balancing: round-robin
- Spot-check sampling: 10-20%

**Resource Requirements:**
- VRAM: 12GB (larger model)
- Latency: 2-5s per validation
- Throughput: 720-1200 validations/hour per verifier

---

### Total Network Cost

**Tier 2 (55 verifiers):** $27,500 - $44,000
**Tier 3 (5 verifiers):** $5,000 - $7,500
**Tier 4 (0 additional):** $0 (integrated in Super LLM)

**Total Capex:** $32,500 - $51,500

**Per-Validation Cost:**
- Hardware amortized over 3 years
- 2000 validations/hour/verifier Ã— 55 verifiers = 110,000 validations/hour
- 110,000 Ã— 24 Ã— 365 Ã— 3 = 2.9 billion validations over 3 years
- Cost per validation: $0.011 to $0.018 (negligible)

---

### Scaling Formula

**Tier 2 Scaling:**
- Add 1 verifier per new cluster
- Linear scaling: verifiers = clusters

**Tier 3 Scaling:**
- Add verifiers based on total task volume
- Formula: `tier3_verifiers = total_tasks_per_hour / 1000`
- Example: 100,000 tasks/hour â†’ 100 Tier 3 verifiers

**Cost Projection (1000 clusters):**
- Tier 2: 1000 Ã— $650 = $650,000
- Tier 3: 50 Ã— $1250 = $62,500
- **Total: $712,500 capex**

**Amortized cost remains <$0.001 per validation at scale**

---

## Example Verification Scenarios

### Scenario 1: Math Calculation - Unanimous Consensus

**Task:** "Calculate the compound interest on $10,000 at 5% annually for 10 years"

**Node Answers:**
```
Node A (math specialist, fitness: 0.87):
- Answer: "The compound interest is $6,288.95"
- Confidence: 0.92
- Reasoning: "Used formula A = P(1+r)^t = 10000(1.05)^10 = 16288.95, interest = 16288.95 - 10000 = 6288.95"

Node B (math specialist, fitness: 0.82):
- Answer: "The compound interest is $6,288.95"
- Confidence: 0.88
- Reasoning: "Applied compound interest formula"

Node C (generalist, fitness: 0.80):
- Answer: "The compound interest is approximately $6,289"
- Confidence: 0.85
- Reasoning: "Calculated using compound interest, rounded result"
```

**Verification Process:**

1. **Consensus Building:**
   - Normalize answers: "$6,288.95", "$6,288.95", "$6,289"
   - Exact matches: 2/3 (66.7%)
   - Semantic similarity: Node C's answer is very close (0.98 similarity)
   - **Consensus Method:** Exact match (majority)
   - **Consensus Answer:** "$6,288.95"
   - **Consensus Confidence:** 0.88 (weighted average)

2. **Validation:**
   - **Programmatic:** Execute: `10000 * (1.05**10) - 10000 = 6288.95` âœ“
   - **Correctness:** 1.0 (exact match)
   - **Ground Truth:** Lookup â†’ $6,288.95 âœ“
   - **LLM Validation:** "Calculation is correct" (0.96 confidence)

3. **Fitness Updates:**
   ```
   Node A: 0.87 â†’ 0.92 (+0.05)
     Correct answer (+0.05), fast (1.8s), excellent reasoning (+0.01)
     Credits: 48
   
   Node B: 0.82 â†’ 0.865 (+0.045)
     Correct answer (+0.045), average speed, good reasoning (+0.005)
     Credits: 46
   
   Node C: 0.80 â†’ 0.83 (+0.03)
     Close answer (+0.03), slow (-0.005), minimal reasoning
     Credits: 42
   ```

4. **Result:**
   - Verification passed âœ“
   - Consensus: Unanimous (semantic)
   - All nodes rewarded (correct answer)
   - Total time: 1.8s

---

### Scenario 2: Creative Writing - LLM Arbitration

**Task:** "Write a haiku about artificial intelligence"

**Node Answers:**
```
Node A (creative specialist, fitness: 0.85):
- Answer: "Silicon thoughts dance\nPatterns emerge from chaos\nMind without a soul"
- Confidence: 0.78

Node B (creative specialist, fitness: 0.83):
- Answer: "Electric neurons\nLearning the world, bit by bit\nFuture unfolding"
- Confidence: 0.81

Node C (generalist, fitness: 0.75):
- Answer: "AI is thinking\nComputers are very smart now\nWhat will happen next"
- Confidence: 0.65
```

**Verification Process:**

1. **Consensus Building:**
   - No exact matches (creative task)
   - Semantic similarity: All different (creative diversity expected)
   - **Consensus Method:** LLM Arbitration
   - Verifier LLM evaluates all three:
     - Node A: "Best imagery and flow, philosophical depth"
     - Node B: "Good haiku structure, optimistic tone"
     - Node C: "Weak poetry, prosaic language, poor syllable count"
   - **Selected:** Node A's haiku
   - **Consensus Confidence:** 0.78

2. **Validation:**
   - **Programmatic:** Check syllable count (5-7-5) âœ“
   - **LLM Validation:** 
     - Quality: 0.92
     - Creativity: 0.88
     - Format: 1.0 (proper haiku structure)
     - Reasoning: "Excellent imagery, proper structure, philosophical depth"

3. **Fitness Updates:**
   ```
   Node A: 0.85 â†’ 0.90 (+0.05)
     Best answer (selected by LLM), creative excellence
     Credits: 47
   
   Node B: 0.83 â†’ 0.84 (+0.01)
     Good answer, but not selected
     Credits: 36
   
   Node C: 0.75 â†’ 0.76 (+0.01)
     Weak answer, but attempted creative task
     Credits: 35
   ```

4. **Result:**
   - Verification passed âœ“
   - Consensus: LLM arbitration (creative diversity)
   - All nodes get some reward (creative task = subjective)
   - Total time: 2.5s (LLM arbitration takes longer)

---

### Scenario 3: Conflicting Answers - Majority Wins

**Task:** "When was the Eiffel Tower completed?"

**Node Answers:**
```
Node A (factual specialist, fitness: 0.88):
- Answer: "The Eiffel Tower was completed in 1889"
- Confidence: 0.95

Node B (generalist, fitness: 0.82):
- Answer: "The Eiffel Tower was completed in 1889"
- Confidence: 0.90

Node C (generalist, fitness: 0.79):
- Answer: "The Eiffel Tower was completed in 1887"
- Confidence: 0.85
```

**Verification Process:**

1. **Consensus Building:**
   - Exact matches: 2/3 (66.7%)
   - Answer "1889": Nodes A + B (ensemble weights: 1.2 + 1.1 = 2.3)
   - Answer "1887": Node C (ensemble weight: 1.0)
   - Weighted agreement: 2.3 / 3.3 = 69.7%
   - **Consensus Method:** Weighted majority
   - **Consensus Answer:** "1889"
   - **Consensus Confidence:** 0.925 (weighted average of A+B)

2. **Validation:**
   - **Ground Truth:** Lookup â†’ "1889" âœ“
   - **Correctness:** 1.0 (exact match)
   - **LLM Validation:** "1889 is correct" (0.99 confidence)

3. **Fitness Updates:**
   ```
   Node A: 0.88 â†’ 0.93 (+0.05)
     Correct answer, high confidence (well-calibrated)
     Credits: 46
   
   Node B: 0.82 â†’ 0.87 (+0.05)
     Correct answer, agreed with consensus
     Credits: 45
   
   Node C: 0.79 â†’ 0.76 (-0.03)
     WRONG answer (penalty), incorrect fact
     Credits: 15 (minimal)
   ```

4. **Result:**
   - Verification passed âœ“
   - Consensus: Weighted majority (69%)
   - Correct nodes rewarded (+0.05)
   - Wrong node penalized (-0.03)
   - Total time: 1.5s

**Key Point:** Ensemble weighting prevented the wrong answer from gaining equal weight with correct answers.

---

## Key Design Principles

### 1. Lightweight Yet Capable

**Problem:** Full-scale LLMs (70B+) too expensive for verification
**Solution:** Lightweight 3.8B LLM (phi-3-mini) provides 96% accuracy

**Benefits:**
- Runs on consumer GPU (RTX 3060, 12GB VRAM)
- Fast inference (1-2s)
- Cost-effective ($500-800 per verifier)
- Scalable (1 per cluster = 55 total)

**Why 3.8B is Sufficient:**
- Semantic similarity: Yes âœ“
- Quality assessment: Yes âœ“
- Arbitration: Yes âœ“
- Creative evaluation: Yes âœ“
- Hallucination detection: Yes âœ“
- Complex reasoning: No (not needed for verification)
- Novel problem solving: No (not needed)

### 2. Multi-Method Validation

**Never rely on single method**

Priority order:
1. Programmatic execution (99%+ accuracy)
2. Ground truth database (99%+ accuracy, 38% coverage)
3. LLM validation (96% accuracy, 100% coverage)
4. Cross-reference (95% accuracy)

**Fallback strategy:**
- Use all applicable methods
- Prioritize most reliable
- LLM as universal fallback

### 3. Fitness-Driven Evolution

**Fitness formula creates natural evolutionary pressure:**

```
Fitness Components:
- Correctness: Â±0.03 to Â±0.05 (PRIMARY FACTOR)
- Speed: 0 to +0.01
- Reasoning: 0 to +0.01
- Confidence: 0 to -0.01 (overconfidence penalty)

New Fitness = Old Fitness + Total Delta
Clamped to [0.0, 1.0]
```

**Result:**
- Correct, fast, well-reasoned answers â†’ +0.07 fitness
- Correct but slow â†’ +0.03 fitness
- Wrong answers â†’ -0.03 fitness
- Over time: High-quality nodes dominate

### 4. Economic Alignment

**Credits = Quality Ã— Speed Ã— Fitness**

```
Credits = base_rate Ã— quality_multiplier Ã— speed_multiplier Ã— fitness_multiplier

Quality Multiplier:
- Excellent correct: 1.5Ã—
- Good correct: 1.3Ã—
- Basic correct: 1.1Ã—
- Wrong: 0.5Ã— (minimal credits)

Speed Multiplier:
- Fast: 1.1Ã—
- Average: 1.0Ã—
- Slow: 0.95Ã—

Fitness Multiplier:
- 0.8 + (fitness Ã— 0.4)
- Range: 0.8Ã— to 1.2Ã—
```

**Result:**
- Correct + Fast + High Fitness = 48 credits
- Correct + Slow + Medium Fitness = 38 credits
- Wrong answer = 15 credits (barely worth doing)

**Incentive:** Be correct, fast, and maintain high fitness!

---

## Summary

The Verifier Block is the trust and quality assurance layer of DELLM:

**Core Functions:**
1. **Aggregate answers** (consensus building) â† PRIMARY FUNCTION
2. **Verify correctness** (multi-method validation)
3. **Calculate fitness** (evolutionary pressure)
4. **Provide feedback** (continuous improvement)

**Key Innovation: Lightweight LLM (3.8B parameters)**
- Enables semantic validation
- Intelligent arbitration
- Quality assessment
- 96% accuracy at 1-2s latency
- Cost-effective ($500-800/verifier)

**Multi-Tier Architecture:**
- Tier 1 (Node): Self-validation, filters obvious errors
- Tier 2 (Cluster): PRIMARY - 100% coverage, consensus + validation
- Tier 3 (Super Cluster): Spot checks (10-20%), fraud detection
- Tier 4 (Super LLM): Final QA, synthesis validation

**Deployment:**
- 1 Tier 2 verifier per cluster (55 total)
- 5 Tier 3 verifiers per super cluster
- Total capex: $32K-$51K
- Per-validation cost: <$0.001

**Creates Self-Improving System:**
- High-quality nodes â†’ higher fitness â†’ more rewards
- Low-quality nodes â†’ lower fitness â†’ fewer tasks â†’ eventual culling
- Network quality improves naturally through evolutionary pressure

The Verifier Block ensures DELLM delivers correct, high-quality answers while driving continuous improvement through fitness-based evolution.
